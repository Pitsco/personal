{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "- title: Favorite Project 1\n",
    "- description: \n",
    "- layout: post\n",
    "- courses: { csa: {week: 25} }\n",
    "- categories: [C4.0]\n",
    "- type: tangibles\n",
    "- comments: true\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Research Paper](https://docs.google.com/document/d/1_575a5hAQGO-7pBOH6bUVmYaq5ABJVvQB9EGZTaP06M/edit?usp=sharing)\n",
    "\n",
    "### [Hugging Face Yelp Dataset](https://huggingface.co/datasets/yelp_review_full)\n",
    "\n",
    "``` cs\n",
    "# Install required libraries\n",
    "!pip install transformers -U\n",
    "!pip install transformers[torch]\n",
    "!pip install accelerate>=0.20.1 -U\n",
    "!pip install datasets\n",
    "!pip install evaluate\n",
    "\n",
    "# Import necessary libraries\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load dataset\n",
    "dataset = load_dataset(\"yelp_review_full\")\n",
    "\n",
    "# Access a sample from the dataset (optional)\n",
    "dataset[\"train\"][50000]\n",
    "\n",
    "# Tokenization\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
    "\n",
    "# Tokenization function\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "\n",
    "# Apply tokenization to the dataset\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Create small training and evaluation datasets\n",
    "small_train_dataset = tokenized_datasets[\"train\"].shuffle(seed=42).select(range(25000))\n",
    "small_eval_dataset = tokenized_datasets[\"test\"].shuffle(seed=42).select(range(25000))\n",
    "\n",
    "# Initialize the model for sequence classification\n",
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"bert-base-cased\", num_labels=5)\n",
    "\n",
    "# Configure training arguments\n",
    "from transformers import TrainingArguments, Trainer\n",
    "training_args = TrainingArguments(output_dir=\"test_trainer\", evaluation_strategy=\"epoch\", num_train_epochs=3)\n",
    "\n",
    "# Import necessary modules for metric computation\n",
    "import numpy as np\n",
    "import evaluate\n",
    "\n",
    "# Load the desired metric (e.g., accuracy)\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "# Define function to compute evaluation metrics\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)\n",
    "\n",
    "# Initialize the Trainer with the model, training arguments, datasets, and metric computation function\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=small_train_dataset,\n",
    "    eval_dataset=small_eval_dataset,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# Start training\n",
    "trainer.train()\n",
    "```\n",
    "\n",
    "## Roberta:\n",
    "Training Loss: This is the error calculated on the training dataset during each iteration (or epoch) of the training process. It represents how well the model is fitting the training data. The goal during training is to minimize this loss, which means the model is learning to make more accurate predictions on the training set.\n",
    "\n",
    "Validation Loss: This is the error calculated on a separate validation dataset during each iteration (or epoch) of the training process. The validation dataset is not used for training; it's held out from the training process and used solely for evaluating the model's performance on unseen data. The validation loss provides an estimate of how well the model is generalizing to new, unseen data. If the validation loss starts to increase while the training loss is decreasing, it indicates that the model is overfitting to the training data and may not generalize well to new data.\n",
    "\n",
    "### Unsuccessful:\n",
    "![image](https://github.com/Pitsco/personal/assets/89278326/ec6ce49c-f4c6-4d10-b4e3-0788e674280e)\n",
    "\n",
    "### Sucessful:\n",
    "![image](https://github.com/Pitsco/personal/assets/89278326/f8a09af5-4b58-4566-a11b-ab0cea0ff15f)\n",
    "\n",
    "## Training Graphs (For Comparison):\n",
    "![image](https://github.com/Pitsco/personal/assets/89278326/9e8b4d48-2b09-435f-a888-8bba5dfd93b5)\n",
    "\n",
    "# Additions To Project:\n",
    "### Creating a System: \n",
    "- One of the project requirements is to create a system that tracks students' pathways through computer science and provides recommendations. \n",
    "- Understanding sentiment analysis techniques can be valuable in developing algorithms to analyze student feedback or sentiment towards different courses or pathways.\n",
    "\n",
    "### Technology Utilization: \n",
    "- The paper explores the use of BERT models, which are based on transformer architectures, for sentiment analysis. \n",
    "- This aligns with the requirement to utilize technologies like ML and AI in the project.\n",
    "\n",
    "### Analysis and Evaluation: \n",
    "- The paper presents results from experiments comparing the performance of different BERT models. \n",
    "- This demonstrates the ability to analyze and evaluate different approaches or models, which is crucial for determining the effectiveness of the tracking system and other components of the project.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
